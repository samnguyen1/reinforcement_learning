{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.plot_utils import load_results\n",
    "from scipy import stats\n",
    "\n",
    "# Load all monitor results\n",
    "df = load_results(\"logs/monitor\")\n",
    "if df.empty:\n",
    "    print(\"No data found for analysis.\")\n",
    "    exit(0)\n",
    "\n",
    "# Parse algorithm, environment, and seed from file path for grouping\n",
    "# Assuming file path format: logs/monitor/<algorithm>_<env_id>_seed<seed>/monitor.csv\n",
    "df['run_id'] = df['file'].apply(lambda x: os.path.basename(os.path.dirname(x)))  # e.g., ppo_HalfCheetah-v5_seed0\n",
    "df[['algorithm', 'env_id', 'seed']] = df['run_id'].str.extract(r'([^_]+)_([^_]+-v\\d+)_seed(\\d+)')\n",
    "df['seed'] = df['seed'].astype(int)\n",
    "\n",
    "# Compute AUC (area under curve) or other summary metrics for each run\n",
    "results = []  # to collect summary for each run\n",
    "for (alg, env, seed), group in df.groupby(['algorithm', 'env_id', 'seed']):\n",
    "    # Sort episodes by time to ensure correct order\n",
    "    group = group.sort_values(by='t')\n",
    "    # Compute area under learning curve: integrate episode reward over training timesteps\n",
    "    # Use trapezoidal rule on (t, cumulative reward). Alternatively, use total reward collected.\n",
    "    auc = np.trapz(y=group['r'].values, x=group['t'].values)\n",
    "    # Also compute final performance (mean of last 10 episodes)\n",
    "    final_mean = group['r'].tail(10).mean()\n",
    "    results.append({\n",
    "        'algorithm': alg,\n",
    "        'env_id': env,\n",
    "        'seed': seed,\n",
    "        'auc': auc,\n",
    "        'final_mean_reward': final_mean\n",
    "    })\n",
    "stats_df = pd.DataFrame(results)\n",
    "\n",
    "# Print out mean and std of AUC and final rewards per algorithm-env\n",
    "summary = stats_df.groupby(['algorithm', 'env_id']).agg({'auc': ['mean','std'], 'final_mean_reward': ['mean','std']})\n",
    "print(\"Performance Summary (mean ± std):\")\n",
    "for (alg, env), sub in stats_df.groupby(['algorithm','env_id']):\n",
    "    auc_mean = sub['auc'].mean(); auc_std = sub['auc'].std()\n",
    "    final_mean = sub['final_mean_reward'].mean(); final_std = sub['final_mean_reward'].std()\n",
    "    print(f\"{alg.upper()} on {env}: AUC = {auc_mean:.2f} ± {auc_std:.2f}, Final 10-ep Reward = {final_mean:.2f} ± {final_std:.2f}\")\n",
    "\n",
    "# Example statistical test: Compare PPO vs SAC on HalfCheetah for final performance\n",
    "algo1, algo2 = \"ppo\", \"sac\"\n",
    "env = \"HalfCheetah-v5\"\n",
    "data1 = stats_df[(stats_df['algorithm']==algo1) & (stats_df['env_id']==env)]['final_mean_reward']\n",
    "data2 = stats_df[(stats_df['algorithm']==algo2) & (stats_df['env_id']==env)]['final_mean_reward']\n",
    "if not data1.empty and not data2.empty:\n",
    "    t_stat, p_val = stats.ttest_ind(data1, data2, equal_var=False)  # Welch’s t-test\n",
    "    print(f\"\\nT-test comparing final performance of {algo1.upper()} vs {algo2.upper()} on {env}:\")\n",
    "    print(f\"t = {t_stat:.3f}, p-value = {p_val:.3f}\")\n",
    "else:\n",
    "    print(f\"Not enough data to compare {algo1} vs {algo2} on {env}.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
